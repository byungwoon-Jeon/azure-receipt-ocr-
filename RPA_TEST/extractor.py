###Initializing runtime enviroment###
import sys
from pathlib import Path

script_path = Path(__file__).resolve()

for parent in script_path.parents:
    if parent.name in ("src", "DEX", "PEX"):
        app_path = str(parent)
        break

if not app_path:
    raise FileNotFoundError(f"ERR")

if app_path not in sys.path:
    sys.path.append(app_path)

from rpa.ai.idp.util import idp_setup_env, idp_utils
from logru import logger

working_path: dict = idp_setup_env.initialize_working_paths(script_path.parent)

idp_utils.setup_logger(log_level="DEBUG",log_path=working_path["idp_log_file_path"])

import logging

###User Define Function###
import os
import re
import json
import tomlkit
from datetime import datetime 

from db_master import query_data_by_date, insert_postprocessed_result
from pre_pre_process import run_pre_pre_process    # Integrated pre-processing + YOLO
from doc_process import run_azure_ocr
from post_process import post_process_and_save
from typing import Optional

#
#
def write_fail_and_insert(duser_input: dict,
    						base: dict,
                            code: str,
                            message: str,
                            attach_file:Optional[str]=None,
                            receipt_index:Optional[str]=None):
	
    now_str=datetime.now().strftime("%Y-%m-%d %H-%M-%S")
    fiid = base.get("FIID")
    line_index = base.get("LINE_INDEX")
    r_idx = receipt_index if receipt_index is not None else base.get("RECEIPT_INDEX")
    
    summary = {
    	"FIID":fiid,
        "LINE_INDEX":line_index,
        "COMMON_YN":base.get("COMMON_YN"),
        "GUBUN" : base.get("GUBUN"),
        "ATTACH_FILE":attach_file,
        "COUNTRY":None, "RECEIPT_TYPE" : None, "MERCHANT_NAME":None,"MERCHANT_PHONE_NO":None,
        "DELIVERY_ADDR":None, "TRANSACTION_DATE":None, "TRANSACTION_TIME":None,
        "TOTAL_AMOUNT":None, "SUMTOTAL_AMOUNT":None, "TAX_AMOUNT":None, "BIZ_NO":None,
        RESULT_CODE:code,
        RESULT_MESSAGE:message,
        CREATE_DATE:now_str,
        UPDATE_DATE:now_str,
        "CONTENTS":None
    }
    
    os.makedirs(duser_input["idp_error_dir"], exit_ok=True)
    fail_name = f"fail_{fiid}_{line_index}_{r_idx if r_idx is not None else 0}_post.json"
    fail_path = os.path.join(duser_input["idp_error_dir"],fail_name)
    with open(fail_path, "w", encoding="utf-8") as f :
    	json.dump({"summary":summary, "items":[]}, f, ensure_asci=False, indent=2)
    
    try:
        inset_postprocessed_reuslt(fail_path,duser_input)
    except Exception as e:
        logger.error("error")    

#
#
def execute_worker(record: dict, duser_input: dict):
    """
    í•˜ë‚˜ì˜ DB ë ˆì½”ë“œì— ëŒ€í•´ ì „ì²´ OCR íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ì „ì²˜ë¦¬(ë‹¤ìš´ë¡œë“œ ë° YOLO í¬ë¡­), Azure OCR ì¸ì‹, í›„ì²˜ë¦¬ ë° DB ì €ì¥ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©°, ê° ë‹¨ê³„ì˜ ê²°ê³¼ì— ë”°ë¼ ì˜¤ë¥˜ ì‹œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.

    ì…ë ¥:
    - record (dict): ì²˜ë¦¬í•  ë‹¨ì¼ ë ˆì½”ë“œ (FIID, LINE_INDEX, GUBUN, ATTACH_FILE, FILE_PATH ë“± í¬í•¨).
    - duser_input (dict): íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì— í•„ìš”í•œ ì„¤ì • ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ (DB ì—°ê²°, ê²½ë¡œ, Azure OCR í‚¤ ë“±).

    ì¶œë ¥:
    - None: ì²˜ë¦¬ëŠ” ë¶€ìˆ˜ íš¨ê³¼(íŒŒì¼ ì €ì¥, DB ì…ë ¥)ë¡œ ì´ë£¨ì–´ì§€ë©°, í•¨ìˆ˜ ìì²´ëŠ” ê°’ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤)
    """
    logger.info(f"[ì‹œì‘] process_single_record - FIID={record.get('FIID')}, LINE_INDEX={record.get('LINE_INDEX')}")
    try:
     #  #ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤í–‰ (ë‹¤ìš´ë¡œë“œ + í¬ë¡­)
        cropped_list = run_pre_pre_process(duser_input, record)

		# ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨
        if not cropped_list:
            for key, r_idx, c_yn in [("ATTACH_FILE", 1, "N"), ("FILE_PATH",1,"Y")]:
                url = record.get(key)
                if not url:
                    continue
                write_fail_and_insert(
                	duser_input=duser_input,
                    base={"FIID":record.get("FIID"),
                    "LINE_INDEX":record.get("LINE_INDEX"),
                    "GUBUN":record.get("GUBUN"),
                    "COMMON_YN":c_yn,
                    "RECEIPT_INDEX":r_idx},
                    code="500",
                    message="ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨",
                    attach_file=url,
                    receipt_index=r_idx
                    )
                return

        for cropped in cropped_list:
            if "RESULT_CODE" in cropped:
                logger.warning(f"[SKIP] YOLO ì˜¤ë¥˜ ë°œìƒ: {cropped}")
                write_fail_and_insert(
                	duser_input=duser_input,
                    base={"FIID":cropped.get("FIID"),
                    "LINE_INDEX":cropped.get("LINE_INDEX"),
                    "GUBUN":cropped.get("GUBUN"),
                    "COMMON_YN":cropped.get("COMMON_YN"),
                    "RECEIPT_INDEX":cropped.get("RECEIPT_INDEX")},
                    code=cropped.get("RESULT_CODE", 500),
                    message=cropped.get("RESULT_MESSAGE", "YOLO ë‹¨ê³„ ì˜¤ë¥˜"),
                    attach_file=cropped.get("source_url"),
                    receipt_index=cropped.get("RECEIPT_INDEX")
                )
                continue

            # OCR ì‹¤í–‰ âœ… ìˆ˜ì • ì½”ë“œ:
            ocr_result = run_azure_ocr(duser_input, cropped)
            if ocr_result.get("RESULT_CODE") == "AZURE_ERR":
                logger.warning(f"[ERROR] Azure OCR ì‹¤íŒ¨ â†’ ì˜¤ë¥˜ summary ì €ì¥ ì‹œë„")

                now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                error_summary = {
                    "FIID": cropped.get("FIID"),
                    "LINE_INDEX": cropped.get("LINE_INDEX"),
                    "RECEIPT_INDEX": cropped.get("RECEIPT_INDEX"),
                    "COMMON_YN": cropped.get("COMMON_YN"),
                    "GUBUN": cropped.get("GUBUN"),
                    "ATTACH_FILE": record.get("ATTACH_FILE"),
                    "COUNTRY": None, "RECEIPT_TYPE": None, "MERCHANT_NAME": None, "MERCHANT_PHONE_NO": None,
                    "DELIVERY_ADDR": None, "TRANSACTION_DATE": None, "TRANSACTION_TIME": None,
                    "TOTAL_AMOUNT": None, "SUMTOTAL_AMOUNT": None, "TAX_AMOUNT": None, "BIZ_NO": None,
                    "RESULT_CODE": ocr_result.get("RESULT_CODE"),
                    "RESULT_MESSAGE": ocr_result.get("RESULT_MESSAGE"),
                    "CREATE_DATE": now_str,
                    "UPDATE_DATE": now_str
                }

                error_result_path = os.path.join(
                    duser_input["post_json_dir"],
                    f"fail_{error_summary['FIID']}_{error_summary['LINE_INDEX']}_{error_summary['RECEIPT_INDEX']}_post.json"
                )
                with open(error_result_path, "w", encoding="utf-8") as f:
                    json.dump({"summary": error_summary, "items": []}, f, ensure_ascii=False, indent=2)

                insert_postprocessed_result(error_result_path, duser_input)
                continue

            # í›„ì²˜ë¦¬ JSON ê²½ë¡œ êµ¬ì„±
            json_path = os.path.join(
                duser_input["idp_azure_dir"],
                f"{os.path.splitext(os.path.basename(cropped['file_path']))[0]}.ocr.json"
            )

            # í›„ì²˜ë¦¬ ì‹¤í–‰
            post_json_path = post_process_and_save(
                {**duser_input, "idp_postprocess_dir": duser_input["idp_postprocess_dir"]},
                {**cropped, "json_path": json_path, "ATTACH_FILE": cropped.get("source_url")}
            )

            # DB ì €ì¥
            insert_postprocessed_result(post_json_path, duser_input)

    except Exception as e:
        logger.error(f"[FATAL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - FIID={record.get('FIID')}: {e}", exc_info=True)
    logger.info(f"[ì¢…ë£Œ] process_single_record - FIID={record.get('FIID')}, LINE_INDEX={record.get('LINE_INDEX')}")

#
#
def _adapter_excute_worker(params:dict):
    record = params["record"]
    duser_input = params["duser_input"]
    retrun execute_worker(record, duser_input)

#
#
def das_process_setup(duser_input:dict) -> dict:
    try :
        logger.debug(f"DAS í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì„¤ì • ì‹œì‘")
        
        idp_filedrop_dir = duser_input["idp_filedrop_dir"]    
        idp_output_files_dir = duser_input["idp_output_files_dir"]
        idp_log_file_path = duser_input["idp_log_file_path"]
        
        sub_folder_name = datetime.now().strtime("%Y%m%d")
        
        idp_workspace_dir = os.path.join(idp_filedrop_dir, sub_folder_name)
        idp_preprocess_dir = os.path.join(idp_workspace_dir, "PreProcess")
        idp_docprocess_dir = os.path.join(idp_workspace_dir, "DocProcess")
        idp_postprocess_dir = os.path.join(idp_workspace_dir, "PostProcess")
        
        idp_rawfile_dir = os.path.join(idp_workspace_dir, "RawFile")
        idp_mergerdoc_dir = os.path.join(idp_prerpocess_dir, "MergeDoc")
        idp_cropped_dir = os.path.join(idp_preprocess_dir, "Cropped")
        idp_azure_dir = os.path.join(idp_docprocess_dir, "Azure")
        idp_error_dir = os.path.join(idp_docprocess_dir, "Error")
        if not os.path.exists(idp_workspace_dir):
            os.mkdir(idp_workspace_dir)
        if not os.path.exists(idp_preprocess_dir):
            os.mkdir(idp_preprocess_dir)
        if not os.path.exists(idp_docprocess_dir):
            os.mkdir(idp_docprocess_dir)
        if not os.path.exists(idp_postprocess_dir):
            os.mkdir(idp_postprocess_dir)
            
        if not os.path.exists(idp_rawfile_dir):
            os.mkdir(idp_rawfile_dir)
        if not os.path.exists(idp_mergedoc_dir):
            os.mkdir(idp_mergedoc_dir)
        if not os.path.exists(idp_cropped_dir):
            os.mkdir(idp_cropped_dir)
        if not os.path.exists(idp_azure_dir):
            os.mkdir(idp_azure_dir)
        if not os.path.exists(idp_error_dir):
            os.mkdir(idp_error_dir)     
    except Exception:
        logger("error")

#
#
def execute(duser_input: dict):
    """
    ì§€ì •í•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  DB ë ˆì½”ë“œë¥¼ ì¡°íšŒí•˜ì—¬ OCR íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ê° ë ˆì½”ë“œë¥¼ ë³„ë„ì˜ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬í•˜ë©°, ì²˜ë¦¬í•  ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.

    ì…ë ¥:
    - duser_input (dict): íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° DB ì—°ê²° ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬. (sqlalchemy_conn, target_date ë“±ê³¼ OCR/YOLO ê´€ë ¨ ì„¤ì • í¬í•¨)

    ì¶œë ¥:
    - None: ì²˜ë¦¬ ì™„ë£Œ í›„ í•¨ìˆ˜ëŠ” ì•„ë¬´ ê°’ë„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ê³¼ì • ì¤‘ ë¡œê·¸ë¡œ ì§„í–‰ ìƒí™©ì„ ê¸°ë¡í•©ë‹ˆë‹¤)
    """
    logger.info("[ì‹œì‘] run_wrapper")
    
    duser_input = {**duser_input,**working_paths}
    duser_input = das_process_setup(duser_input)
    
    data_records = query_data_by_date(duser_input)
    if not data_records:
        logger.info("ğŸ“­ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ì´ {len(data_records)}ê±´ ì²˜ë¦¬ ì‹œì‘")
    
    func_params_list = [
    	{"record":rec, "duser_input":duser_input}
        for rec in data_records
    ]
    
    idp_utils.run_in_multi_thread(
    	target_func=adapter_excute_worker,
        func_params_list=func_params_list,
    )
    
    logger.info("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    logger.info("[ì¢…ë£Œ] run_wrapper")

if __name__ == "__main__":
	duser_input = {
    	"SystemName" : "DAS01",
        "ccrParams":{
        	"targetDate" : "2025-09-03"
        }
    }
	execute(duser_input)
