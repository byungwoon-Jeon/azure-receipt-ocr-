# --- ì–´ëŒ‘í„°: íšŒì‚¬ ìŠ¤ë ˆë“œ ìœ í‹¸ìš© (_adapter_execute_worker) ---
def _adapter_execute_worker(params: dict) -> dict:
    """
    idp_utils.run_in_multi_threadìš© ì–´ëŒ‘í„°.
    ì…ë ¥: {"idp_item": dict, "duser_input": dict}
    ì¶œë ¥: execute_workerì˜ ë°˜í™˜ dict
    """
    return execute_worker(params["idp_item"], params["duser_input"])

#
# --- execute: ì•„ì´í…œ ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬ ---
def execute(duser_input: dict) -> str:
    logger.info("[ì‹œì‘] extractor.execute (ì•„ì´í…œ ê¸°ë°˜)")

    # 1) ì›Œí‚¹ ê²½ë¡œ ë³‘í•© + í™˜ê²½ ì…‹ì—… (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    try:
        wps = idp_setup_env.initialize_working_paths(script_path.parent)
        duser_input = {**duser_input, **wps}
    except Exception:
        logger.exception("[WARN] working paths ì´ˆê¸°í™” ì‹¤íŒ¨ - ê³„ì† ì§„í–‰")

    vr = validate_required_fields(duser_input)
    if vr["has_error"]:
        ret = {
            "callback_url": None, "req_no": None, "cmd_id": None,
            "has_error": True, "error_message": vr["error_message"], "error_fields": vr["error_fields"],
            "idp_items": []
        }
        logger.warning(f"ì…ë ¥ê°’ ê²€ì¦ ì‹¤íŒ¨: {vr['error_message']}")
        return json.dumps(ret, ensure_ascii=False, indent=2)

    duser_input = das_process_setup(duser_input)

    # 2) DBì—ì„œ ëŒ€ìƒ ë ˆì½”ë“œ ì¡°íšŒ (ì›ë³¸ ìœ ì§€)
    try:
        data_records = query_data_by_date(duser_input)
    except Exception as e:
        logger.exception("[FATAL] DB ì¡°íšŒ ì‹¤íŒ¨")
        ret = {
            "callback_url": None, "req_no": None, "cmd_id": None,
            "has_error": True, "error_message": f"DB ì¡°íšŒ ì‹¤íŒ¨: {e}", "error_fields": None,
            "idp_items": []
        }
        return json.dumps(ret, ensure_ascii=False, indent=2)

    if not data_records:
        ret = {
            "callback_url": None, "req_no": None, "cmd_id": None,
            "has_error": False, "error_message": None, "error_fields": None,
            "idp_items": []
        }
        logger.info("ğŸ“­ ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")
        return json.dumps(ret, ensure_ascii=False, indent=2)

    # 3) (ë©€í‹°ìŠ¤ë ˆë“œ) ì „ì²˜ë¦¬ ëë‚œ ì•„ì´í…œ ìƒì„±
    #    NOTE: generate_idp_items() ì•ˆì—ì„œ run_pre_processë¥¼ ë³‘ë ¬ë¡œ ëŒë ¤ OCR ëŒ€ìƒ ì•„ì´í…œì„ ë§Œë“¤ì–´ì˜¨ë‹¤ëŠ” ì „ì œ
    try:
        from pre_processing import generate_idp_items  # ë„¤ê°€ ë§Œë“  í•¨ìˆ˜
    except Exception:
        # ëª¨ë“ˆ ê²½ë¡œê°€ ë‹¤ë¥´ë©´ ì—¬ê¸° importë§Œ ë§ì¶°ì¤˜
        from pre_pre_process import generate_idp_items  # ì˜ˆë¹„

    idp_items = generate_idp_items(duser_input, data_records)
    if not idp_items:
        ret = {
            "callback_url": None, "req_no": None, "cmd_id": None,
            "has_error": False, "error_message": None, "error_fields": None,
            "idp_items": []
        }
        logger.info("ğŸ“­ ìƒì„±ëœ OCR ëŒ€ìƒ ì•„ì´í…œì´ ì—†ìŠµë‹ˆë‹¤.")
        return json.dumps(ret, ensure_ascii=False, indent=2)

    # 4) (ë©€í‹°ìŠ¤ë ˆë“œ) ì•„ì´í…œ ë‹¨ìœ„ ì›Œì»¤ ì‹¤í–‰: OCR â†’ í›„ì²˜ë¦¬ â†’ DB
    func_params_list = [
        {"idp_item": idp_item, "duser_input": duser_input}
        for idp_item in idp_items
    ]

    results = idp_utils.run_in_multi_thread(
        func=_adapter_execute_worker,
        func_params_list=func_params_list,
    )

    # 5) ê²°ê³¼ ì§‘ê³„ í›„ ë°˜í™˜
    ret = {
        "callback_url": None, "req_no": None, "cmd_id": None,
        "has_error": any(r.get("has_error") for r in results),
        "error_message": None, "error_fields": None,
        "idp_items": results
    }
    logger.info("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì•„ì´í…œ ê¸°ë°˜)")
    return json.dumps(ret, ensure_ascii=False, indent=2)
    
    
    # --- execute_worker: (ì „ì²˜ë¦¬ ì œì™¸) OCR â†’ í›„ì²˜ë¦¬ â†’ DB ---
def execute_worker(idp_item: dict, duser_input: dict) -> dict:
    """
    ì…ë ¥: generate_idp_items()ê°€ ë§Œë“  'ì•„ì´í…œ' (ì „ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœ)
      í•„ìˆ˜ í•„ë“œ ì˜ˆ:
        - FIID, LINE_INDEX, RECEIPT_INDEX, COMMON_YN, GUBUN
        - file_path (OCR ëŒ€ìƒ ê²½ë¡œ)
        - RESULT_CODE, RESULT_MESSAGE (ì „ì²˜ë¦¬ ê²°ê³¼ ì½”ë“œ/ë©”ì‹œì§€; ì„ íƒ)
    ì¶œë ¥: ì²˜ë¦¬ ê²°ê³¼ë¥¼ í¬í•¨í•œ idp_item (has_error, error_message ê°±ì‹  ë“±)
    """
    fiid = idp_item.get("FIID")
    line_idx = idp_item.get("LINE_INDEX")
    rcp_idx = idp_item.get("RECEIPT_INDEX")
    logger.info(f"[ì‹œì‘] execute_worker item={fiid}-{line_idx}-{rcp_idx}")

    # (ì˜µì…˜) ì‹¤í–‰ ì´ë ¥ ì‹œì‘ ê¸°ë¡ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ í˜¸ì¶œ
    # _write_history_start(idp_item, duser_input)

    try:
        # 0) ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤íŒ¨ ì•„ì´í…œ ê°€ë“œ (ìˆë‹¤ë©´ ê³§ë°”ë¡œ ì‹¤íŒ¨ ìš”ì•½ ìƒì„±)
        pre_code = str(idp_item.get("RESULT_CODE", "200"))
        if pre_code != "200":
            msg = idp_item.get("RESULT_MESSAGE") or "ì „ì²˜ë¦¬ ì‹¤íŒ¨"
            logger.warning(f"[SKIP] ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì•„ì´í…œ: {fiid}-{line_idx}-{rcp_idx} / {msg}")
            try:
                write_fail_and_insert(
                    duser_input=duser_input,
                    base=idp_item,
                    code=pre_code,
                    message=msg,
                    attach_file=None,
                    receipt_index=idp_item.get("RECEIPT_INDEX"),
                )
            except Exception:
                logger.exception("[WARN] ì‹¤íŒ¨ ìš”ì•½ ê¸°ë¡ ì¤‘ ì˜ˆì™¸ (ì „ì²˜ë¦¬ ì‹¤íŒ¨)")
            idp_item["has_error"] = True
            idp_item["error_message"] = msg
            return idp_item

        # 1) OCR (Azure)
        if run_azure_ocr is None:
            raise RuntimeError("run_azure_ocr í•¨ìˆ˜ import ì‹¤íŒ¨")

        ocr_in = {**idp_item}  # í•„ìš” ì‹œ í•„ë“œ ì¶•ì•½/í™•ì¥ ê°€ëŠ¥
        ocr_out = run_azure_ocr(duser_input, ocr_in)

        if isinstance(ocr_out, dict) and str(ocr_out.get("RESULT_CODE")) in ("AZURE_ERR", "500"):
            msg = ocr_out.get("RESULT_MESSAGE", "Azure OCR ì‹¤íŒ¨")
            logger.warning(f"[ERROR] Azure OCR ì‹¤íŒ¨: {fiid}-{line_idx}-{rcp_idx} / {msg}")
            try:
                write_fail_and_insert(
                    duser_input=duser_input,
                    base=idp_item,
                    code=str(ocr_out.get("RESULT_CODE", "AZURE_ERR")),
                    message=msg,
                    attach_file=None,
                    receipt_index=idp_item.get("RECEIPT_INDEX"),
                )
            except Exception:
                logger.exception("[WARN] ì‹¤íŒ¨ ìš”ì•½ ê¸°ë¡ ì¤‘ ì˜ˆì™¸ (OCR ì‹¤íŒ¨)")
            idp_item["has_error"] = True
            idp_item["error_message"] = msg
            return idp_item

        # 2) í›„ì²˜ë¦¬
        if post_process_and_save is None:
            raise RuntimeError("post_process_and_save í•¨ìˆ˜ import ì‹¤íŒ¨")

        # OCR JSON ê²½ë¡œ ì¶”ì •/ê²°ì •
        azure_dir = duser_input.get("idp_azure_dir") or os.path.join(duser_input["idp_workspace_dir"], "Azure")
        os.makedirs(azure_dir, exist_ok=True)
        try:
            base_name = Path(idp_item.get("file_path") or "").stem or f"{fiid}_{line_idx}_{rcp_idx}"
        except Exception:
            base_name = f"{fiid}_{line_idx}_{rcp_idx}"
        ocr_json_path = os.path.join(azure_dir, f"{base_name}.ocr.json")

        post_json_path = post_process_and_save(
            {**duser_input, "idp_postprocess_dir": duser_input.get("idp_postprocess_dir")},
            {**idp_item, "json_path": ocr_json_path}
        )

        # 3) DB ì €ì¥
        if insert_postprocessed_result is None:
            raise RuntimeError("insert_postprocessed_result í•¨ìˆ˜ import ì‹¤íŒ¨")
        insert_postprocessed_result(post_json_path, duser_input)

        idp_item["has_error"] = False
        idp_item["error_message"] = None
        logger.info(f"[ì„±ê³µ] execute_worker item={fiid}-{line_idx}-{rcp_idx}")

    except Exception as e:
        logger.error(f"[FATAL] execute_worker ì˜ˆì™¸: {e}", exc_info=True)
        idp_item["has_error"] = True
        idp_item["error_message"] = str(e)

    finally:
        # (ì˜µì…˜) ì‹¤í–‰ ì´ë ¥ ì¢…ë£Œ ê¸°ë¡
        # _write_history_end(idp_item)
        logger.info(f"[ì¢…ë£Œ] execute_worker item={fiid}-{line_idx}-{rcp_idx}")

    return idp_item
    
# ì‹¤íŒ¨ ìš”ì•½ íŒŒì¼ ìƒì„± + DB ì…ë ¥ (idp_item ê¸°ë°˜ ë‹¨ì¼ ì¸ì)
from typing import Dict, Any, Optional
import os, json, logging
from datetime import datetime

logger = logging.getLogger("EXTRACTOR")

def write_fail_and_insert(duser_input: Dict[str, Any], idp_item: Dict[str, Any]) -> None:
    """
    idp_itemì— ì´ë¯¸ ëª¨ë“  ì •ë³´(FIID, LINE_INDEX, RECEIPT_INDEX, COMMON_YN, GUBUN,
    RESULT_CODE, RESULT_MESSAGE, ATTACH_FILE/source_url/file_path)ê°€ ìˆë‹¤ê³  ê°€ì •.
    - ì‹¤íŒ¨ ìš”ì•½ JSON ìƒì„± (./Error ë˜ëŠ” duser_input['idp_error_dir'])
    - insert_postprocessed_result(...)ë¡œ DB ì…ë ¥ (ìˆìœ¼ë©´)
    ì˜ˆì™¸ ë°œìƒ ì‹œ ë¡œê¹… í›„ ì•ˆì „íˆ ë°˜í™˜.
    """
    try:
        now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # ì‹ë³„ì/ë©”íƒ€
        fiid: Optional[str] = idp_item.get("FIID")
        line_index: Optional[int] = idp_item.get("LINE_INDEX")
        r_idx: int = int(idp_item.get("RECEIPT_INDEX", 0))
        common_yn: Optional[str] = idp_item.get("COMMON_YN")
        gubun: Optional[str] = idp_item.get("GUBUN")

        # ì‹¤íŒ¨ ì½”ë“œ/ë©”ì‹œì§€ (ê¸°ë³¸ê°’ ì•ˆì „í™”)
        result_code: str = str(idp_item.get("RESULT_CODE", "500"))
        result_msg: str = idp_item.get("RESULT_MESSAGE") or idp_item.get("error_message") or "Processing Failed"

        # ì²¨ë¶€/ê²½ë¡œ ì •ë³´ ìš°ì„ ìˆœìœ„
        attach_file = (
            idp_item.get("ATTACH_FILE")
            or idp_item.get("source_url")
            or idp_item.get("file_path")
        )

        # ì—ëŸ¬ ë””ë ‰í„°ë¦¬
        err_dir = duser_input.get("idp_error_dir") \
                  or os.path.join(duser_input.get("idp_workspace_dir", "."), "Error")
        os.makedirs(err_dir, exist_ok=True)

        # ì‹¤íŒ¨ ìš”ì•½ ë‚´ìš©
        summary = {
            "FIID": fiid,
            "LINE_INDEX": line_index,
            "COMMON_YN": common_yn,
            "GUBUN": gubun,
            "ATTACH_FILE": attach_file,
            "COUNTRY": None,
            "RECEIPT_TYPE": None,
            "MERCHANT_NAME": None,
            "MERCHANT_PHONE_NO": None,
            "DELIVERY_ADDR": None,
            "TRANSACTION_DATE": None,
            "TRANSACTION_TIME": None,
            "TOTAL_AMOUNT": None,
            "SUMTOTAL_AMOUNT": None,
            "TAX_AMOUNT": None,
            "BIZ_NO": None,
            "RESULT_CODE": result_code,
            "RESULT_MESSAGE": result_msg,
            "CREATE_DATE": now_str,
            "UPDATE_DATE": now_str,
            "CONTENTS": None,
        }

        # íŒŒì¼ ì €ì¥
        fail_name = f"fail_{fiid}_{line_index}_{r_idx}_post.json"
        fail_path = os.path.join(err_dir, fail_name)

        with open(fail_path, "w", encoding="utf-8") as f:
            json.dump({"summary": summary, "items": []}, f, ensure_ascii=False, indent=2)

        # DB ì…ë ¥ ì‹œë„ (ëª¨ë“ˆì´ ë¡œë“œë˜ì–´ ìˆë‹¤ë©´)
        try:
            # ì „ì—­/ìƒìœ„ ìŠ¤ì½”í”„ì— ì´ë¯¸ import ë˜ì–´ ìˆë‹¤ê³  ê°€ì •:
            # from db_master import insert_postprocessed_result
            if 'insert_postprocessed_result' in globals() and callable(globals()['insert_postprocessed_result']):
                globals()['insert_postprocessed_result'](fail_path, duser_input)
            else:
                logger.warning("[WARN] insert_postprocessed_result ë¯¸ë¡œë“œ - DB ì…ë ¥ ìƒëµ")
        except Exception:
            logger.exception("[WARN] ì‹¤íŒ¨ ìš”ì•½ DB ì…ë ¥ ì‹¤íŒ¨")

        logger.info(f"[FAIL-SAVED] {fail_path}")

    except Exception:
        logger.exception("[WARN] ì‹¤íŒ¨ ìš”ì•½ íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
    
# pre_pre_process.py (ë˜ëŠ” ì „ì²˜ë¦¬ ëª¨ë“ˆ íŒŒì¼)
from typing import Dict, Any, List, Optional
import os
import logging

logger = logging.getLogger("PRE_PROCESS")

def _make_fail_crop(data_record: Dict[str, Any], msg: str) -> Dict[str, Any]:
    """ì „ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ í•­ìƒ í¬ë¡­ ì•„ì´í…œ 1ê°œë¥¼ ë°˜í™˜í•˜ê¸° ìœ„í•œ í—¬í¼"""
    return {
        "FIID": data_record.get("FIID"),
        "LINE_INDEX": data_record.get("LINE_INDEX"),
        "RECEIPT_INDEX": data_record.get("RECEIPT_INDEX", 0),
        "COMMON_YN": data_record.get("COMMON_YN"),
        "GUBUN": data_record.get("GUBUN"),
        "file_path": None,                  # ì „ì²˜ë¦¬ ì‚°ì¶œë¬¼ ì—†ìŒ
        "source_url": data_record.get("ATTACH_FILE") or data_record.get("FILE_PATH"),
        "RESULT_CODE": "500",
        "RESULT_MESSAGE": f"Preprocess failed: {msg}",
    }

def run_pre_process(duser_input: Dict[str, Any], data_record: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    ì „ì²˜ë¦¬ ì„±ê³µ ì‹œ: list[dict] (ê° dictëŠ” ì •ìƒ í¬ë¡­, RESULT_CODE ê¸°ë³¸ '200')
    ì „ì²˜ë¦¬ ì‹¤íŒ¨/ì˜ˆì™¸/ë¹ˆ ê²°ê³¼ ì‹œ: ì‹¤íŒ¨ í¬ë¡­ 1ê±´ì„ ë‹´ì€ list[dict] ë°˜í™˜
    """
    try:
        # --- ê¸°ì¡´ ì „ì²˜ë¦¬ ë¡œì§ (ë‹¤ìš´ë¡œë“œ/ë³‘í•©/í¬ë¡­ ë“±) ---
        # ì•„ë˜ ë‘ ì¤„ì€ "ì˜ˆì‹œ" ìë¦¬. ë„¤ ì›ë˜ êµ¬í˜„ì„ í˜¸ì¶œ/ì‚½ì…í•´.
        # cropped_list = <ì›ë˜_ì „ì²˜ë¦¬_êµ¬í˜„>(duser_input, data_record)
        # cropped_list: List[Dict[str, Any]]  # ì „ì œ

        cropped_list = []  # <-- ì—¬ê¸°ëŠ” ë„¤ ê¸°ì¡´ êµ¬í˜„ìœ¼ë¡œ ëŒ€ì²´

        # ë¹ˆ ê²°ê³¼ë„ ì‹¤íŒ¨ë¡œ ê°„ì£¼í•˜ì—¬ 1ê±´ ë°˜í™˜
        if not cropped_list:
            logger.warning(
                f"[PRE] no cropped output: FIID={data_record.get('FIID')}, "
                f"LINE_INDEX={data_record.get('LINE_INDEX')}"
            )
            return [_make_fail_crop(data_record, "no cropped output")]

        # ì •ìƒ ê²°ê³¼ ì •ê·œí™”: í•„ìˆ˜ í‚¤ ì±„ìš°ê³  RESULT_CODE ê¸°ë³¸ '200'
        normalized: List[Dict[str, Any]] = []
        for c in cropped_list:
            result_code = str(c.get("RESULT_CODE", "200"))
            result_msg = c.get("RESULT_MESSAGE")
            normalized.append({
                "FIID": c.get("FIID", data_record.get("FIID")),
                "LINE_INDEX": c.get("LINE_INDEX", data_record.get("LINE_INDEX")),
                "RECEIPT_INDEX": c.get("RECEIPT_INDEX", data_record.get("RECEIPT_INDEX", 1)),
                "COMMON_YN": c.get("COMMON_YN", data_record.get("COMMON_YN")),
                "GUBUN": c.get("GUBUN", data_record.get("GUBUN")),
                "file_path": c.get("file_path"),
                "source_url": c.get("source_url", data_record.get("ATTACH_FILE") or data_record.get("FILE_PATH")),
                "RESULT_CODE": result_code,
                "RESULT_MESSAGE": result_msg,
            })
        return normalized

    except Exception as e:
        logger.exception(
            f"[PRE] exception: FIID={data_record.get('FIID')}, "
            f"LINE_INDEX={data_record.get('LINE_INDEX')}, err={e}"
        )
        # ì˜ˆì™¸ë„ ì‹¤íŒ¨ í¬ë¡­ 1ê±´ìœ¼ë¡œ ë°˜í™˜
        return [_make_fail_crop(data_record, str(e))]
