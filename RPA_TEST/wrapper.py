import os
import json
import logging
import tomllib
from concurrent.futures import ThreadPoolExecutor, as_completed
from sqlalchemy import create_engine

from db_master import query_data_by_date, insert_postprocessed_result
from pre_pre_process import run_pre_pre_process    # Integrated pre-processing + YOLO
from doc_process import run_azure_ocr
from post_process import post_process_and_save

def process_single_record(record: dict, in_params: dict):
    """
    í•˜ë‚˜ì˜ DB ë ˆì½”ë“œì— ëŒ€í•´ ì „ì²´ OCR íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ì „ì²˜ë¦¬(ë‹¤ìš´ë¡œë“œ ë° YOLO í¬ë¡­), Azure OCR ì¸ì‹, í›„ì²˜ë¦¬ ë° DB ì €ì¥ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë©°, ê° ë‹¨ê³„ì˜ ê²°ê³¼ì— ë”°ë¼ ì˜¤ë¥˜ ì‹œ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.

    ì…ë ¥:
    - record (dict): ì²˜ë¦¬í•  ë‹¨ì¼ ë ˆì½”ë“œ (FIID, LINE_INDEX, GUBUN, ATTACH_FILE, FILE_PATH ë“± í¬í•¨).
    - in_params (dict): íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì— í•„ìš”í•œ ì„¤ì • ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ (DB ì—°ê²°, ê²½ë¡œ, Azure OCR í‚¤ ë“±).

    ì¶œë ¥:
    - None: ì²˜ë¦¬ëŠ” ë¶€ìˆ˜ íš¨ê³¼(íŒŒì¼ ì €ì¥, DB ì…ë ¥)ë¡œ ì´ë£¨ì–´ì§€ë©°, í•¨ìˆ˜ ìì²´ëŠ” ê°’ì„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì˜¤ë¥˜ ë°œìƒ ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ë¡œê·¸ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤)
    """
    logger.info(f"[ì‹œì‘] process_single_record - FIID={record.get('FIID')}, LINE_INDEX={record.get('LINE_INDEX')}")
    try:
        # ì „ì²˜ë¦¬ ë‹¨ê³„ ì‹¤í–‰ (ë‹¤ìš´ë¡œë“œ + í¬ë¡­)
        cropped_list = run_pre_pre_process(in_params, record)

        for cropped in cropped_list:
            if "RESULT_CODE" in cropped:
                logger.warning(f"[SKIP] YOLO ì˜¤ë¥˜ ë°œìƒ: {cropped}")
                continue

            # OCR ì‹¤í–‰
            ocr_result = run_azure_ocr(in_params, cropped)
            if ocr_result.get("RESULT_CODE") == "AZURE_ERR":
                logger.warning(f"[SKIP] Azure OCR ì˜¤ë¥˜ ë°œìƒ: {ocr_result}")
                continue

            # í›„ì²˜ë¦¬ JSON ê²½ë¡œ êµ¬ì„±
            json_path = os.path.join(
                in_params["ocr_json_dir"],
                f"{os.path.splitext(os.path.basename(cropped['file_path']))[0]}.ocr.json"
            )

            # í›„ì²˜ë¦¬ ì‹¤í–‰
            post_json_path = post_process_and_save(
                {**in_params, "postprocess_output_dir": in_params["post_json_dir"]},
                {**cropped, "json_path": json_path, "ATTACH_FILE": record.get("ATTACH_FILE")}
            )

            # DB ì €ì¥
            insert_postprocessed_result(post_json_path, in_params)

    except Exception as e:
        logger.error(f"[FATAL] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - FIID={record.get('FIID')}: {e}", exc_info=True)
    logger.info(f"[ì¢…ë£Œ] process_single_record - FIID={record.get('FIID')}, LINE_INDEX={record.get('LINE_INDEX')}")

def run_wrapper(in_params: dict):
    """
    ì§€ì •í•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  DB ë ˆì½”ë“œë¥¼ ì¡°íšŒí•˜ì—¬ OCR íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    ê° ë ˆì½”ë“œë¥¼ ë³„ë„ì˜ ìŠ¤ë ˆë“œë¡œ ì²˜ë¦¬í•˜ë©°, ì²˜ë¦¬í•  ë ˆì½”ë“œê°€ ì—†ìœ¼ë©´ í•¨ìˆ˜ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.

    ì…ë ¥:
    - in_params (dict): íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° DB ì—°ê²° ì •ë³´ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬. (sqlalchemy_conn, target_date ë“±ê³¼ OCR/YOLO ê´€ë ¨ ì„¤ì • í¬í•¨)

    ì¶œë ¥:
    - None: ì²˜ë¦¬ ì™„ë£Œ í›„ í•¨ìˆ˜ëŠ” ì•„ë¬´ ê°’ë„ ë°˜í™˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ê³¼ì • ì¤‘ ë¡œê·¸ë¡œ ì§„í–‰ ìƒí™©ì„ ê¸°ë¡í•©ë‹ˆë‹¤)
    """
    logger.info("[ì‹œì‘] run_wrapper")
    data_records = query_data_by_date(in_params)
    if not data_records:
        logger.info("ğŸ“­ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"ì´ {len(data_records)}ê±´ ì²˜ë¦¬ ì‹œì‘")
    max_workers = in_params.get("max_workers", 4)
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_single_record, rec, in_params) for rec in data_records]
        for future in as_completed(futures):
            future.result()
    logger.info("âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
    logger.info("[ì¢…ë£Œ] run_wrapper")

if __name__ == "__main__":
    # Load database configuration from TOML file
    with open("Module_config_dex.toml", "rb") as f:
        config = tomllib.load(f)
    db_conf = config.get("database", config.get("hana", {}))  # support [database] or [hana] section
    hana_user = db_conf["user"]; hana_pass = db_conf["password"]
    hana_host = db_conf["host"]; hana_port = db_conf["port"]
    hana_conn_str = f"hdbcli://{hana_user}:{hana_pass}@{hana_host}:{hana_port}"
    engine = create_engine(hana_conn_str)

    # Set up unified logging (file + console)
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(log_dir, "pipeline.log")
    logging.basicConfig(
        level=logging.INFO, 
        format="%(asctime)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(log_file, mode="a", encoding="utf-8"),
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger("WRAPPER")
    logger.setLevel(logging.INFO)  # Use INFO level or as specified

    # Prepare parameters for pipeline
    in_params = {
        "sqlalchemy_conn": engine.connect(),
        # target_date not provided will default to yesterday in query_data_by_date if needed
        "target_date": "2025-07-09",  # Example date; could be omitted or set as needed
        "azure_endpoint": "https://<your-endpoint>.cognitiveservices.azure.com/",
        "azure_key": "<your-azure-key>",
        "output_dir": "./output",
        "preprocessed_dir": "./preprocessed",
        "cropped_dir": "./cropped",
        "ocr_json_dir": "./ocr_json",
        "post_json_dir": "./post_json",
        "error_json_dir": "./error_json",
        "yolo_model_path": "./yolo/best.pt",
        "max_workers": 4
    }

    # Run the pipeline
    run_wrapper(in_params)

    # Clean up database connection
    in_params["sqlalchemy_conn"].close()
